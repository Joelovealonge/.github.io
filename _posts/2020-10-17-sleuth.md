---
title: sleuth
tags:

  - SpringCloud
---

### spring-cloud-sletch

对于一个大型的微服务架构系统，会有哪些问题呢？

如何串联调用链，快速定位问题

如何理清微服务之间的依赖关系

如何进行各个微服务接口的分析

如何跟踪业务流的处理



### sleuth

spring cloud sleuth 为spring cloud 提供了分布式跟踪的解决方案，它大量借用了Google Dapper、Twitter Zipkin 和 Apache Htrace 的设计，sleuth借用了Dapper的术语。



span（跨度）：

​	基本工作单元。span用一个64位的id唯一标识。除id外，span还包含其他数据，例如描述、时间戳、键值对的注释（标签），spanId、span父Id等。 span被启动和停止时，记录了时间信息。初始化span被称为“rootSpan”，该span的id和trace的id相等。

trace（跟踪）：

​	一组共享“rootspan”的span组成的树状结构称为trace，trace也用一个64位的id唯一标识，trace中的所有span都共享该trace的id。

annotation（标注）：

​	annotation用来记录事件的存在，其中，核心annotation用来定义请求的开放和结束。

CS（client sent 客户端发送）：

​	客户端发起一个请求，该annotation表明完成请求处理（当响应发挥客户端时）。如果用SS减去SR时间戳，就能得到服务器端处理请求所需的时间。

CR（client received客户端接收）：

​	span结束的标识。客户端成功接收到服务器端的响应。如果CR减去CS时间戳，就能得到从客户端发送请求到服务器端响应所需的时间



spring cloud sleuth可以追踪10中类型的组件：async、hystrix、messaging、websocket、rxjava、scheduling、web（spring mvc controller， servlet）、webclient（Spring RestTemplate）、Feign、Zuul



![image-20201029091839709](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029091839709.png)

上图是官方给出的一张微服务调用链示意图。



#### sleuth 整合Zipkin实现分布式链路跟踪

Zipkin是Twitter开源的风不是跟踪系统，基于Dapper的论文设计而来。她的主要功能是手机系统的时序数据，从而跟踪微服务架构的系统延时等问题。Zipkin还提供了一个非常友好的界面，来帮助分析跟踪数据。



sleuth对于分布式链路的跟踪仅仅是一些数据的记录，这些数据我们认为来读和处理难免会太麻烦了，所以我们一般把这种数据上角给Zipkin server 来统一处理。



##### Zipkin server

在zipkin2.7.x以后便不再支持自定义服务器需要使用官方的版本或者Docker，

要使用官方版本请查看官网：官网地址： https://zipkin.io/

我在github上下载的2.22.0版本

![image-20201029110319329](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029110319329.png)

用java启动后：

![image-20201029110406455](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029110406455.png)

可以看到访问地址为 http://localhost:9411/zipkin/

![image-20201029110504774](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029110504774.png)

目前来讲，我们肯定是查不到数据的，我们把自己的sleuth微服务和zipkin整合，并把数据上传到zipkin server



#### sleuth微服务整合zipkin

在我们的user-feign、power3001、power2中，

加入以下依赖：

```yaml
spring:
  zipkin:
    base-url: http://localhost:9411
  sleuth:
    sampler:
      probability: 1.0 #request采样的数量 默认是0.1 也就是10%，即采取以10%的请求数据
```

然后启动微服务并模拟一次调用链，我这里是用user微服务调用了power微服务（注意每个微服务都需要和zipkin整合）

调用完成后，我们区zipkin server页面去看看：

我这里调用了4次：

![image-20201029114633372](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029114633372.png)

正常情况下：我们可以看到调用链以及各个过程的时间：

![image-20201029115451776](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029115451776.png)

轮询到错误的那个power后，也看一看出来：

![image-20201029115738083](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029115738083.png)



### zipkin server 数据持久化问题

当zipkin重启后我们的分布式链路数据全部清空了。

因为zipkin server 默认数据是存储在内存中，所以当你服务重启之后内存自然而然也就清空了。



####  使用Elasticsearch 做数据持久化

我们这里使用es来做数据持久化，当然也可以用ELK来做。

es支持本文不做讨论，后面我会整理es的只是。

es下载地址： https://www.elastic.co/cn/downloads/elasticsearch



下载完是个压缩包 解压出来 打开bin目录 找到elasticsearch.bat文件启动

等他启动一会儿然后在页面上输入localhost:9200看见如下信息说明Elasticsearch 启动好了：

![image-20201029123755053](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029123755053.png)

我们的zipkin server jar 如何使用elasticsearch做存储呢？

```java
java -jar zipkin-server-2.22.0-exec.jar --STORAGE_TYPE=elasticsearch ES_HOSTS=http://localhost:9200
```

具体请查看官网：

https://github.com/openzipkin/zipkin/tree/master/zipkin-server#elasticsearch-storage

可以看到：我们的索引已自动创建成功，数据也已经有了

![image-20201029124807289](https://cdn.jsdelivr.net/gh/joelovealonge/noteimgs/image-20201029124807289.png)

至此 zipkin的数据便和Elasticsearch整合起来了，现在再启动zipkin server 并且存储几条数据， 就算重启， 数据还会在上面。

